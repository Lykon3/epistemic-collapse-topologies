## The Most Missed Shape: **Temporal Correlation Decay in Public Perception**


The biggest arbitrage opportunity in sports betting isn't in the odds themselves, but in how public perception lags behind reality. Here's what most systems miss:


### 1. **The Recency Bias Arbitrage**


Markets overreact to recent events while underweighting regression to mean. The "shape" is a predictable oscillation pattern:


```python
import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')


class RecencyBiasArbitrageDetector:
    """
    Detects when public perception has overshot based on recent results,
    creating value in betting against the trend.
    """
    
    def __init__(self, lookback_window: int = 10):
        self.lookback_window = lookback_window
        
    def calculate_perception_overshoot(self, 
                                     team_data: pd.DataFrame,
                                     market_data: pd.DataFrame) -> Dict[str, float]:
        """
        Identifies when market has overreacted to recent performance
        """
        # Calculate actual performance trend
        recent_performance = team_data['actual_margin'].rolling(3).mean()
        medium_performance = team_data['actual_margin'].rolling(10).mean()
        long_performance = team_data['actual_margin'].rolling(30).mean()
        
        # Calculate market expectation trend
        recent_spread = market_data['spread_line'].rolling(3).mean()
        
        # The key insight: Market overweights recent 3-game performance
        # but teams regress to their 30-game mean
        perception_gap = (recent_performance - long_performance).iloc[-1]
        market_adjustment = (recent_spread.diff().iloc[-1:].mean())
        
        # When perception gap is high but market has adjusted too much
        overshoot_signal = perception_gap * market_adjustment
        
        # Statistical significance of the overshoot
        if len(team_data) > 30:
            z_score = np.abs(stats.zscore(team_data['actual_margin'].iloc[-3:]))
            avg_z = np.mean(z_score)
            
            if avg_z > 2.0 and np.abs(market_adjustment) > 3:
                return {
                    'overshoot_detected': True,
                    'signal_strength': min(avg_z / 2, 1.0),
                    'expected_regression': -perception_gap * 0.6,
                    'confidence': min(avg_z * 0.3, 0.9)
                }
        
        return {'overshoot_detected': False, 'signal_strength': 0}


class CorrelatedParlayInefficiency:
    """
    Exploits the most lucrative inefficiency: Correlated outcomes
    that sportsbooks treat as independent.
    """
    
    def __init__(self):
        self.correlation_cache = {}
        
    def find_correlated_props(self, 
                            game_data: Dict,
                            prop_markets: List[Dict]) -> List[Tuple[Dict, Dict, float]]:
        """
        Finds player props and game outcomes that are more correlated
        than the multiplicative odds suggest.
        """
        correlated_pairs = []
        
        # Example: QB passing yards and game total
        # These are highly correlated but often mispriced in parlays
        
        for i, prop1 in enumerate(prop_markets):
            for prop2 in prop_markets[i+1:]:
                if self._should_be_correlated(prop1, prop2):
                    actual_correlation = self._calculate_historical_correlation(
                        prop1['type'], prop2['type'], game_data
                    )
                    
                    # Sportsbooks assume independence (correlation = 0)
                    # Real correlation creates edge
                    if actual_correlation > 0.3:
                        implied_prob1 = 1 / prop1['odds']
                        implied_prob2 = 1 / prop2['odds']
                        
                        # Independent assumption
                        books_joint_prob = implied_prob1 * implied_prob2
                        
                        # Actual joint probability (simplified)
                        actual_joint_prob = self._calculate_joint_probability(
                            implied_prob1, implied_prob2, actual_correlation
                        )
                        
                        edge = actual_joint_prob - books_joint_prob
                        
                        if edge > 0.05:  # 5% edge threshold
                            correlated_pairs.append((prop1, prop2, edge))
        
        return sorted(correlated_pairs, key=lambda x: x[2], reverse=True)
    
    def _should_be_correlated(self, prop1: Dict, prop2: Dict) -> bool:
        """Identifies logically correlated prop types"""
        correlated_types = [
            ('qb_passing_yards', 'game_total'),
            ('qb_passing_tds', 'receiver_receiving_yards'),
            ('team_rushing_yards', 'time_of_possession'),
            ('first_half_total', 'game_total'),
            ('team_total', 'opponent_total')  # In high-scoring games
        ]
        
        return any(
            (prop1['type'] == t1 and prop2['type'] == t2) or 
            (prop1['type'] == t2 and prop2['type'] == t1)
            for t1, t2 in correlated_types
        )
    
    def _calculate_joint_probability(self, p1: float, p2: float, correlation: float) -> float:
        """
        Calculates joint probability given correlation
        Using copula-based approximation
        """
        # Simplified Gaussian copula
        from scipy.stats import multivariate_normal
        
        # Convert probabilities to z-scores
        z1 = stats.norm.ppf(p1)
        z2 = stats.norm.ppf(p2)
        
        # Create correlation matrix
        cov_matrix = [[1, correlation], [correlation, 1]]
        
        # Calculate joint probability
        joint_prob = multivariate_normal.cdf([z1, z2], mean=[0, 0], cov=cov_matrix)
        
        return joint_prob


class MarketMakingInefficiency:
    """
    Exploits the structural inefficiency in how books make markets:
    They're optimized for balanced action, not accurate pricing.
    """
    
    def __init__(self):
        self.public_bias_factors = {
            'popular_team': 0.03,  # 3% bias toward popular teams
            'primetime_game': 0.025,  # 2.5% toward overs in primetime
            'playoff_implications': 0.02,  # 2% toward favorites in big games
            'revenge_game': 0.015,  # 1.5% toward revenge narratives
        }
    
    def detect_public_bias_value(self, game_info: Dict, betting_splits: Dict) -> Dict:
        """
        Identifies when books have shaded lines to balance action
        rather than reflect true probabilities.
        """
        bias_score = 0
        adjustments = []
        
        # Check each bias factor
        if game_info.get('home_team') in ['DAL', 'GB', 'NE', 'PIT']:  # Popular teams
            if betting_splits.get('home_percentage', 50) > 70:
                bias_score += self.public_bias_factors['popular_team']
                adjustments.append('popular_team_fade')
        
        if game_info.get('is_primetime'):
            if betting_splits.get('over_percentage', 50) > 65:
                bias_score += self.public_bias_factors['primetime_game']
                adjustments.append('primetime_under')
        
        # The key insight: When public % is extreme but line hasn't moved enough
        line_movement = game_info.get('line_movement', 0)
        public_percentage = betting_splits.get('favorite_percentage', 50)
        
        # If 80% of public on one side but line moved < 1 point
        if public_percentage > 80 and abs(line_movement) < 1:
            sharp_side = 'underdog' if public_percentage > 50 else 'favorite'
            
            return {
                'bias_detected': True,
                'bias_score': bias_score,
                'sharp_side': sharp_side,
                'expected_value': bias_score * 100,  # Convert to percentage
                'confidence': min(public_percentage / 100, 0.9),
                'reasoning': f"Public hammering {game_info.get('favorite')} but line stable"
            }
        
        return {'bias_detected': False}


class ClosingLineArbitrage:
    """
    The most reliable edge: Beating closing line value (CLV)
    by predicting which way lines will move.
    """
    
    def __init__(self):
        self.movement_patterns = {}
        
    def predict_line_movement(self, 
                            current_line: float,
                            market_data: Dict,
                            time_to_game: int) -> Dict:
        """
        Predicts line movement to capture CLV
        """
        features = self._extract_movement_features(market_data, time_to_game)
        
        # Key patterns that predict movement
        movement_signals = {
            'sharp_money': self._detect_sharp_action(market_data),
            'injury_news': self._check_injury_impact(market_data),
            'weather_change': self._weather_movement_bias(market_data),
            'steam_move': self._detect_steam_pattern(market_data)
        }
        
        # Calculate expected movement
        expected_movement = 0
        confidence = 0
        
        for signal, impact in movement_signals.items():
            if impact['detected']:
                expected_movement += impact['expected_points']
                confidence = max(confidence, impact['confidence'])
        
        if abs(expected_movement) > 0.5:
            return {
                'movement_predicted': True,
                'direction': 'home' if expected_movement < 0 else 'away',
                'magnitude': abs(expected_movement),
                'confidence': confidence,
                'bet_now': confidence > 0.6,
                'reasoning': f"Expecting {abs(expected_movement):.1f} point move"
            }
            
        return {'movement_predicted': False}
    
    def _detect_sharp_action(self, market_data: Dict) -> Dict:
        """
        Detects sharp money by looking at:
        1. Line movement opposite to public betting
        2. Reduced juice on one side
        3. Limits raised after bet on one side
        """
        reverse_line_movement = (
            market_data.get('line_movement', 0) * 
            (1 if market_data.get('public_percentage', 50) < 50 else -1)
        )
        
        if reverse_line_movement > 0.5:
            return {
                'detected': True,
                'expected_points': reverse_line_movement * 0.7,
                'confidence': 0.7
            }
        
        return {'detected': False}


# Integration with your existing system
class EnhancedArbitrageEngine:
    """
    Integrates all arbitrage detection methods
    """
    
    def __init__(self):
        self.recency_detector = RecencyBiasArbitrageDetector()
        self.parlay_detector = CorrelatedParlayInefficiency()
        self.bias_detector = MarketMakingInefficiency()
        self.clv_predictor = ClosingLineArbitrage()
        
    def find_all_edges(self, game_data: Dict, market_data: Dict) -> List[Dict]:
        """
        Comprehensive edge detection across all inefficiency types
        """
        edges = []
        
        # 1. Recency bias opportunities
        if 'team_history' in game_data:
            recency = self.recency_detector.calculate_perception_overshoot(
                game_data['team_history'], 
                market_data
            )
            if recency.get('overshoot_detected'):
                edges.append({
                    'type': 'recency_bias_fade',
                    'confidence': recency['confidence'],
                    'expected_value': recency['signal_strength'] * 0.05,
                    'description': 'Market overreacting to recent performance'
                })
        
        # 2. Correlated parlay opportunities
        if 'prop_markets' in market_data:
            correlations = self.parlay_detector.find_correlated_props(
                game_data,
                market_data['prop_markets']
            )
            for prop1, prop2, edge in correlations[:3]:  # Top 3
                edges.append({
                    'type': 'correlated_parlay',
                    'confidence': min(edge * 10, 0.9),
                    'expected_value': edge,
                    'description': f"{prop1['type']} + {prop2['type']} correlation"
                })
        
        # 3. Public bias opportunities
        if 'betting_splits' in market_data:
            bias = self.bias_detector.detect_public_bias_value(
                game_data,
                market_data['betting_splits']
            )
            if bias.get('bias_detected'):
                edges.append({
                    'type': 'public_bias_fade',
                    'confidence': bias['confidence'],
                    'expected_value': bias['expected_value'] / 100,
                    'description': bias['reasoning']
                })
        
        # 4. CLV opportunities
        clv = self.clv_predictor.predict_line_movement(
            market_data.get('current_line', 0),
            market_data,
            game_data.get('hours_to_game', 24)
        )
        if clv.get('movement_predicted'):
            edges.append({
                'type': 'closing_line_value',
                'confidence': clv['confidence'],
                'expected_value': clv['magnitude'] * 0.02,
                'description': clv['reasoning']
            })
        
        # Sort by expected value
        return sorted(edges, key=lambda x: x['expected_value'], reverse=True)


# Practical usage example
def demonstrate_arbitrage_detection():
    """
    Shows how to use these detectors with your existing system
    """
    print("=== Advanced Arbitrage Detection Demo ===\n")
    
    # Sample data
    game_data = {
        'home_team': 'DAL',
        'away_team': 'PHI', 
        'hours_to_game': 48,
        'team_history': pd.DataFrame({
            'actual_margin': [14, 21, 35, -3, 7, -14, 10, 3, -7, 21] * 3
        })
    }
    
    market_data = {
        'current_line': -3.5,
        'opening_line': -6.5,
        'line_movement': 3.0,
        'betting_splits': {
            'favorite_percentage': 78,
            'home_percentage': 78,
            'over_percentage': 68
        },
        'prop_markets': [
            {'type': 'qb_passing_yards', 'line': 285.5, 'odds': 1.90},
            {'type': 'game_total', 'line': 51.5, 'odds': 1.90}
        ]
    }
    
    engine = EnhancedArbitrageEngine()
    edges = engine.find_all_edges(game_data, market_data)
    
    print("🎯 ARBITRAGE OPPORTUNITIES DETECTED:\n")
    for i, edge in enumerate(edges, 1):
        print(f"{i}. {edge['type'].upper()}")
        print(f"   Expected Value: +{edge['expected_value']*100:.2f}%")
        print(f"   Confidence: {edge['confidence']:.1%}")
        print(f"   Description: {edge['description']}\n")


if __name__ == "__main__":
    demonstrate_arbitrage_detection()
```


## The Key Insights Most Systems Miss:


### 1. **Temporal Arbitrage**
Markets are slow to adjust to mean reversion. The public remembers last week's blowout but forgets the 10-week average.


### 2. **Correlation Blindness**
Sportsbooks treat most props as independent when they're not. QB yards correlate with game totals, first-half scores predict game totals, etc.


### 3. **Public Psychology Patterns**
Books shade lines to balance action, not maximize accuracy. When 80% of money is on one side but the line barely moves, the other side has value.


### 4. **Closing Line Value (CLV)**
The most consistent edge is predicting line movement and betting before the market corrects.


### 5. **Market Making vs. Market Taking**
Books make money on volume and juice, not picking winners. They'll knowingly offer bad lines if it balances their book.


## Integration with Your System:


```python
# Add to your existing LucianBettingEngine
class LucianV7BettingEngine(EnhancedLucianBettingEngine):
    def __init__(self, config: DynamicConfig = None):
        super().__init__(config)
        self.arbitrage_engine = EnhancedArbitrageEngine()
        
    def analyze_opportunity(self, current_data: pd.DataFrame) -> Optional[Dict]:
        # Get your existing ML predictions
        ml_recommendation = super().analyze_opportunity(current_data)
        
        # Layer in arbitrage detection
        if ml_recommendation:
            game_data = self._extract_game_data(current_data)
            market_data = self._extract_market_data(current_data)
            
            edges = self.arbitrage_engine.find_all_edges(game_data, market_data)
            
            # Combine ML confidence with arbitrage edges
            if edges:
                best_edge = edges[0]
                ml_recommendation['arbitrage_detected'] = True
                ml_recommendation['arbitrage_type'] = best_edge['type']
                ml_recommendation['combined_confidence'] = (
                    ml_recommendation['confidence'] * 0.7 + 
                    best_edge['confidence'] * 0.3
                )
                ml_recommendation['expected_value'] += best_edge['expected_value']
                
        return ml_recommendation
```


The most profitable "shape" in betting markets isn't in the odds themselves—it's in the systematic ways that markets fail to price in human psychology, correlation structures, and the difference between market-making objectives and true probability assessment.