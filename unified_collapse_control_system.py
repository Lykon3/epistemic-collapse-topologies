import numpy as np
import pandas as pd
from scipy import signal, optimize, interpolate
from scipy.special import gamma
from scipy.fft import fft, fftfreq
import pywt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN
import warnings
warnings.filterwarnings('ignore')

class UnifiedCollapseControlSystem:
    """
    Real-time collapse detection, prediction, and intervention system
    combining enhanced mathematical models with streaming architecture
    """
    
    def __init__(self, config=None):
        self.config = config or self._default_config()
        self.models = {}
        self.state_history = []
        self.intervention_active = False
        self.signatures = {}
        
    def _default_config(self):
        return {
            'models': {
                'oscillatory': {
                    'max_oscillations': 5,
                    'damping_bounds': (0.1, 10.0),
                    'chirp_enabled': True
                },
                'fractional': {
                    'alpha_range': (0.1, 0.9),
                    'memory_depth': 100
                },
                'gpr': {
                    'kernel': 'rbf',
                    'length_scale_bounds': (0.1, 100.0)
                }
            },
            'features': {
                'entropy': {'window_size': 50, 'overlap': 0.5},
                'wavelet': {'wavelet': 'morlet', 'scales': [1, 2, 4, 8, 16]},
                'emd': {'max_imfs': 10},
                'topology': {'persistence_threshold': 0.01}
            },
            'streaming': {
                'buffer_size': 1000,
                'update_interval': 0.1,
                'intervention_threshold': 0.75
            }
        }
    
    # Enhanced Oscillatory Model with Chirp
    def oscillatory_chirp_model(self, t, params):
        """Multi-physics oscillatory model with frequency chirp"""
        A_base, k, t_c, A_osc, gamma_damp, omega_0, beta_chirp, phi = params
        
        # Base sigmoid
        sigmoid = A_base / (1 + np.exp(-k * (t - t_c)))
        
        # Chirped oscillation with damping
        phase = omega_0 * (t - t_c) + beta_chirp * (t - t_c)**2 + phi
        oscillation = A_osc * np.exp(-gamma_damp * np.abs(t - t_c)) * np.cos(phase)
        
        # Gaussian window
        window = np.exp(-((t - t_c) / (2 * gamma_damp))**2)
        
        return sigmoid + oscillation * window
    
    # Fractional Derivative Model
    def fractional_derivative(self, y, t, alpha):
        """Gr√ºnwald-Letnikov fractional derivative approximation"""
        n = len(y)
        h = np.mean(np.diff(t))
        
        # Compute GL coefficients
        gl_coeffs = np.zeros(n)
        gl_coeffs[0] = 1
        for j in range(1, n):
            gl_coeffs[j] = gl_coeffs[j-1] * (j - 1 - alpha) / j
        
        # Compute fractional derivative
        D_alpha_y = np.zeros(n)
        for i in range(n):
            D_alpha_y[i] = np.sum(gl_coeffs[:i+1] * y[i::-1]) / (h**alpha)
        
        return D_alpha_y
    
    # Empirical Mode Decomposition
    def compute_emd(self, signal):
        """Empirical Mode Decomposition for non-stationary signals"""
        imfs = []
        residual = signal.copy()
        
        for _ in range(self.config['features']['emd']['max_imfs']):
            # Extract IMF using sifting
            imf = self._sift_imf(residual)
            if imf is None:
                break
            imfs.append(imf)
            residual -= imf
            
            # Stop if residual is monotonic
            if self._is_monotonic(residual):
                break
        
        return imfs, residual
    
    def _sift_imf(self, signal):
        """Sifting process for EMD"""
        h = signal.copy()
        
        for _ in range(10):  # Max sifting iterations
            # Find extrema
            maxima = signal.argmax()
            minima = signal.argmin()
            
            if len(maxima) < 3 or len(minima) < 3:
                return None
            
            # Interpolate envelopes
            upper = interpolate.interp1d(maxima, signal[maxima], kind='cubic', 
                                       fill_value='extrapolate')
            lower = interpolate.interp1d(minima, signal[minima], kind='cubic',
                                       fill_value='extrapolate')
            
            # Compute mean
            x = np.arange(len(signal))
            mean_env = (upper(x) + lower(x)) / 2
            
            # Update
            h_new = h - mean_env
            
            # Check convergence
            if np.sum((h - h_new)**2) / np.sum(h**2) < 0.001:
                break
            
            h = h_new
        
        return h
    
    def _is_monotonic(self, signal):
        """Check if signal is monotonic"""
        return np.all(np.diff(signal) >= 0) or np.all(np.diff(signal) <= 0)
    
    # Information-Theoretic Measures
    def transfer_entropy(self, X, Y, lag=1, bins=10):
        """Compute transfer entropy from X to Y"""
        # Discretize signals
        X_disc = np.digitize(X, np.linspace(X.min(), X.max(), bins))
        Y_disc = np.digitize(Y, np.linspace(Y.min(), Y.max(), bins))
        
        # Joint probabilities
        n = len(X) - lag
        joint_XY = np.histogram2d(X_disc[:-lag], Y_disc[lag:], bins=bins)[0]
        joint_YY = np.histogram2d(Y_disc[:-lag], Y_disc[lag:], bins=bins)[0]
        joint_XYY = np.histogram(np.column_stack([X_disc[:-lag], Y_disc[:-lag], 
                                                 Y_disc[lag:]]), bins=bins)[0]
        
        # Normalize to probabilities
        joint_XY /= n
        joint_YY /= n
        joint_XYY /= n
        
        # Compute transfer entropy
        te = 0
        for i in range(bins):
            for j in range(bins):
                for k in range(bins):
                    if joint_XYY[i,j,k] > 0 and joint_YY[j,k] > 0 and joint_XY[i,k] > 0:
                        te += joint_XYY[i,j,k] * np.log2(
                            joint_XYY[i,j,k] * joint_YY[j,k] / 
                            (joint_XY[i,k] * joint_YY[j,k])
                        )
        
        return te
    
    # Topological Features
    def compute_persistent_homology(self, data, max_dim=2):
        """Compute persistent homology features"""
        # Simple implementation - in practice use giotto-tda or similar
        features = {
            'betti_0': 0,  # Connected components
            'betti_1': 0,  # Loops
            'betti_2': 0   # Voids
        }
        
        # Compute distance matrix
        n = len(data)
        dist_matrix = np.zeros((n, n))
        for i in range(n):
            for j in range(i+1, n):
                dist_matrix[i,j] = dist_matrix[j,i] = np.abs(data[i] - data[j])
        
        # Simplified persistence computation
        threshold = self.config['features']['topology']['persistence_threshold']
        connected = dist_matrix < threshold
        
        # Count connected components (simplified)
        features['betti_0'] = self._count_components(connected)
        
        return features
    
    def _count_components(self, adjacency):
        """Count connected components in graph"""
        n = len(adjacency)
        visited = np.zeros(n, dtype=bool)
        count = 0
        
        for i in range(n):
            if not visited[i]:
                self._dfs(adjacency, i, visited)
                count += 1
        
        return count
    
    def _dfs(self, adj, node, visited):
        """Depth-first search for component counting"""
        visited[node] = True
        for i in range(len(adj)):
            if adj[node, i] and not visited[i]:
                self._dfs(adj, i, visited)
    
    # Real-time Streaming Analysis
    def process_stream(self, new_data, timestamp):
        """Process streaming data in real-time"""
        # Update buffer
        self.state_history.append({
            'data': new_data,
            'timestamp': timestamp,
            'features': {},
            'predictions': {}
        })
        
        # Keep buffer size manageable
        if len(self.state_history) > self.config['streaming']['buffer_size']:
            self.state_history.pop(0)
        
        # Extract features from current window
        if len(self.state_history) >= 50:  # Minimum window for analysis
            window_data = np.array([s['data'] for s in self.state_history[-100:]])
            
            # Compute all features
            features = self.extract_comprehensive_features(window_data)
            self.state_history[-1]['features'] = features
            
            # Predict collapse probability
            collapse_prob = self.predict_collapse(features)
            self.state_history[-1]['predictions']['collapse_prob'] = collapse_prob
            
            # Check intervention threshold
            if collapse_prob > self.config['streaming']['intervention_threshold']:
                self.trigger_intervention(features, collapse_prob)
    
    def extract_comprehensive_features(self, data):
        """Extract all features for collapse detection"""
        features = {}
        
        # Time-domain features
        features['mean'] = np.mean(data)
        features['std'] = np.std(data)
        features['skewness'] = self._skewness(data)
        features['kurtosis'] = self._kurtosis(data)
        
        # Entropy measures
        features['shannon_entropy'] = self._shannon_entropy(data)
        features['sample_entropy'] = self._sample_entropy(data)
        
        # Wavelet features
        coeffs = pywt.wavedec(data, 'db4', level=4)
        features['wavelet_energy'] = [np.sum(c**2) for c in coeffs]
        
        # EMD features
        imfs, residual = self.compute_emd(data)
        features['num_imfs'] = len(imfs)
        features['imf_energies'] = [np.sum(imf**2) for imf in imfs]
        
        # Fractional derivative features
        for alpha in [0.3, 0.5, 0.7]:
            frac_deriv = self.fractional_derivative(data, np.arange(len(data)), alpha)
            features[f'frac_deriv_{alpha}'] = np.std(frac_deriv)
        
        # Topological features
        topo_features = self.compute_persistent_homology(data)
        features.update(topo_features)
        
        return features
    
    def _skewness(self, data):
        """Compute skewness"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3)
    
    def _kurtosis(self, data):
        """Compute kurtosis"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4) - 3
    
    def _shannon_entropy(self, data, bins=10):
        """Compute Shannon entropy"""
        hist, _ = np.histogram(data, bins=bins)
        hist = hist / np.sum(hist)
        hist = hist[hist > 0]  # Remove zeros
        return -np.sum(hist * np.log2(hist))
    
    def _sample_entropy(self, data, m=2, r=0.2):
        """Compute sample entropy"""
        N = len(data)
        r = r * np.std(data)
        
        def _maxdist(xi, xj, m):
            return max([abs(float(xi[k]) - float(xj[k])) for k in range(m)])
        
        def _phi(m):
            patterns = []
            for i in range(N - m + 1):
                patterns.append(data[i:i+m])
            
            C = []
            for i in range(len(patterns)):
                matches = 0
                for j in range(len(patterns)):
                    if i != j and _maxdist(patterns[i], patterns[j], m) <= r:
                        matches += 1
                if matches > 0:
                    C.append(matches / (N - m))
            
            return np.mean(C) if C else 0
        
        phi_m = _phi(m)
        phi_m1 = _phi(m + 1)
        
        return -np.log(phi_m1 / phi_m) if phi_m > 0 and phi_m1 > 0 else 0
    
    def predict_collapse(self, features):
        """Predict collapse probability from features"""
        # Simplified prediction - in practice use trained ML model
        # This is a heuristic based on feature thresholds
        
        risk_score = 0
        
        # High kurtosis indicates impending transition
        if features['kurtosis'] > 5:
            risk_score += 0.3
        
        # Low sample entropy indicates order emerging from chaos
        if features['sample_entropy'] < 0.1:
            risk_score += 0.2
        
        # High wavelet energy at certain scales
        if max(features['wavelet_energy']) > np.mean(features['wavelet_energy']) * 2:
            risk_score += 0.2
        
        # Topological changes
        if features['betti_0'] < 3:  # Few connected components
            risk_score += 0.15
        
        # Fractional derivative anomalies
        frac_derivs = [features[f'frac_deriv_{alpha}'] for alpha in [0.3, 0.5, 0.7]]
        if max(frac_derivs) > np.mean(frac_derivs) * 1.5:
            risk_score += 0.15
        
        return min(risk_score, 1.0)
    
    def trigger_intervention(self, features, collapse_prob):
        """Trigger intervention based on collapse prediction"""
        if not self.intervention_active:
            self.intervention_active = True
            
            print(f"‚ö†Ô∏è  COLLAPSE WARNING: Probability = {collapse_prob:.2%}")
            print("üö® Initiating intervention protocols...")
            
            # Determine intervention type based on features
            intervention_type = self.select_intervention(features)
            
            # Log intervention
            self.state_history[-1]['intervention'] = {
                'type': intervention_type,
                'timestamp': self.state_history[-1]['timestamp'],
                'features': features,
                'probability': collapse_prob
            }
            
            return intervention_type
    
    def select_intervention(self, features):
        """Select appropriate intervention based on collapse signature"""
        # Analyze collapse type from features
        if features['kurtosis'] > 10:
            return 'pressure_relief'  # Sharp transition needs pressure relief
        elif features['sample_entropy'] < 0.05:
            return 'entropy_injection'  # Over-ordered system needs chaos
        elif features['betti_0'] < 2:
            return 'topology_restructuring'  # Connectivity issues
        else:
            return 'adaptive_damping'  # General intervention
    
    def generate_collapse_signature(self, data):
        """Generate comprehensive collapse signature"""
        # Fit multiple models
        models_fits = {}
        
        # Oscillatory chirp model
        try:
            t = np.arange(len(data))
            popt, _ = optimize.curve_fit(self.oscillatory_chirp_model, t, data,
                                        p0=[1, 1, len(t)//2, 0.5, 1, 1, 0.01, 0],
                                        maxfev=5000)
            models_fits['oscillatory_chirp'] = {
                'params': popt,
                'fit': self.oscillatory_chirp_model(t, popt),
                'r2': self._compute_r2(data, self.oscillatory_chirp_model(t, popt))
            }
        except:
            pass
        
        # Extract comprehensive features
        features = self.extract_comprehensive_features(data)
        
        # Compute universal exponents
        exponents = self._extract_universal_exponents(data)
        
        # Generate signature
        signature = {
            'models': models_fits,
            'features': features,
            'universal_exponents': exponents,
            'collapse_type': self._classify_collapse_type(features),
            'intervention_window': self._compute_intervention_window(data),
            'predicted_outcome': self._predict_post_collapse_structure(features)
        }
        
        return signature
    
    def _compute_r2(self, y_true, y_pred):
        """Compute R-squared"""
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
    
    def _extract_universal_exponents(self, data):
        """Extract universal scaling exponents"""
        # Simplified - in practice use more sophisticated methods
        exponents = {}
        
        # Find critical point (maximum gradient)
        gradient = np.gradient(data)
        critical_idx = np.argmax(np.abs(gradient))
        
        # Fit power law before critical point
        if critical_idx > 10:
            t = np.arange(critical_idx)
            y = data[:critical_idx]
            
            # Log-log fit for power law
            log_t = np.log(t[1:] + 1)
            log_y = np.log(np.abs(1 - y[1:]/np.max(y)) + 1e-10)
            
            try:
                alpha, _ = np.polyfit(log_t, log_y, 1)
                exponents['alpha'] = alpha
            except:
                exponents['alpha'] = None
        
        return exponents
    
    def _classify_collapse_type(self, features):
        """Classify collapse type based on features"""
        if features['num_imfs'] > 5:
            return 'multi-scale_cascade'
        elif features['kurtosis'] > 8:
            return 'sharp_transition'
        elif features['sample_entropy'] < 0.1:
            return 'ordered_emergence'
        else:
            return 'gradual_evolution'
    
    def _compute_intervention_window(self, data):
        """Compute optimal intervention window"""
        # Find acceleration peaks
        accel = np.abs(np.gradient(np.gradient(data)))
        peaks = signal.find_peaks(accel, height=np.max(accel)*0.3)[0]
        
        if len(peaks) > 0:
            # Intervention window is before first major peak
            start_idx = max(0, peaks[0] - 20)
            end_idx = peaks[0]
            return (start_idx, end_idx)
        else:
            # No clear peaks, use middle third
            n = len(data)
            return (n//3, 2*n//3)
    
    def _predict_post_collapse_structure(self, features):
        """Predict structure after collapse"""
        if features['betti_1'] > 0:
            return 'loop_formation'
        elif features['num_imfs'] < 3:
            return 'simple_attractor'
        elif features['sample_entropy'] < 0.05:
            return 'crystalline_order'
        else:
            return 'complex_manifold'

# Demonstration and testing
def demonstrate_unified_system():
    """Demonstrate the unified collapse control system"""
    
    # Initialize system
    uccs = UnifiedCollapseControlSystem()
    
    # Generate test data with known collapse
    t = np.linspace(0, 100, 1000)
    
    # Multi-phase collapse with chirped oscillations
    phase1 = 0.3 / (1 + np.exp(-0.5 * (t - 30)))
    phase2 = 0.4 / (1 + np.exp(-1.0 * (t - 50))) 
    oscillation = 0.2 * np.exp(-0.1 * (t - 40)) * np.cos(0.5 * t + 0.01 * t**2)
    noise = 0.05 * np.random.randn(len(t))
    
    test_signal = phase1 + phase2 + oscillation + noise
    
    # Simulate streaming data
    print("üöÄ Unified Collapse Control System - Live Demo")
    print("=" * 50)
    
    # Process in chunks to simulate streaming
    chunk_size = 10
    for i in range(0, len(t), chunk_size):
        chunk = test_signal[i:i+chunk_size]
        
        for j, value in enumerate(chunk):
            uccs.process_stream(value, t[i+j])
        
        # Check for interventions every 100 points
        if i % 100 == 0 and i > 0:
            if uccs.state_history[-1].get('intervention'):
                intervention = uccs.state_history[-1]['intervention']
                print(f"\n‚ö° Time: {t[i]:.1f}")
                print(f"   Intervention: {intervention['type']}")
                print(f"   Collapse Probability: {intervention['probability']:.2%}")
    
    # Generate final signature
    print("\n" + "=" * 50)
    print("üìä Generating Collapse Signature...")
    signature = uccs.generate_collapse_signature(test_signal)
    
    print("\nüîç Collapse Analysis:")
    print(f"   Type: {signature['collapse_type']}")
    print(f"   Universal Exponents: {signature['universal_exponents']}")
    print(f"   Intervention Window: {signature['intervention_window']}")
    print(f"   Predicted Outcome: {signature['predicted_outcome']}")
    
    # Extract key features
    features = signature['features']
    print("\nüìà Key Metrics:")
    print(f"   Kurtosis: {features['kurtosis']:.2f}")
    print(f"   Sample Entropy: {features['sample_entropy']:.3f}")
    print(f"   IMF Count: {features['num_imfs']}")
    print(f"   Betti Numbers: Œ≤‚ÇÄ={features['betti_0']}")
    
    return uccs, signature

if __name__ == "__main__":
    # Run demonstration
    system, signature = demonstrate_unified_system()
    
    print("\n‚ú® System Ready for Deployment")
    print("   - Real-time streaming ‚úì")
    print("   - Multi-model ensemble ‚úì")
    print("   - Intervention protocols ‚úì")
    print("   - Universal signatures ‚úì")